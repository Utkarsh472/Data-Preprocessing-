{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataPreprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utkarsh472/Data-Preprocessing-/blob/main/DataPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-qiINBQSK2g"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=pd.read_csv('Data.csv')\n",
        "#Important rule of machine learning\n",
        "#We have to select features and dependent variable vetor for our machine learning model\n",
        "#Here Country,Age,salary are features and purchased column is our dependent variable vector\n",
        "X=dataset.iloc[:,:-1].values # matrix of features\n",
        "Y=dataset.iloc[:,-1].values #dependent variable vector\n",
        "#iloc is one of attribute of pandas means to locate indexes\n"
      ],
      "metadata": {
        "id": "yVON-WsUG-b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAN1KmIHMxYU",
        "outputId": "9a02922b-5f9e-4484-d2ee-6ac3dd4431cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzM8bpNmM5s3",
        "outputId": "89753660-2b27-4862-8acf-63e8d902d425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer #SimpleImputer is class ,#Scikit learn(sklearn ) is a preprocessing library,#impute is module of sklearn\n",
        "imputer = SimpleImputer(missing_values=np.nan,strategy='mean') #imputer is the instance of class(object)to replace nan values with mean\n",
        "#now will we apply to matrix features\n",
        "#fit method: this will connect imputer to matrix features or simply will fill up missing values with average of columns\n",
        "imputer.fit(X[:,1:3])\n",
        "#Now we will call transform method from our imputer\n",
        "X[:,1:3]=imputer.transform(X[:,1:3])\n"
      ],
      "metadata": {
        "id": "_13QEqZiM46B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LdmkePZfbwH",
        "outputId": "ec258bed-a62f-4b58-cd22-48c091a3b76f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One Hot Encoding consists of creating binary vectors for each of the countries\n",
        "#For this we are going to use two classes\n",
        "#the first one is column transform, a class from compose module of sklearn\n",
        "#and second class OneHotEncoder from preprocessing module of sklearn\n"
      ],
      "metadata": {
        "id": "eMD4XbIikgT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct=ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])],remainder='passthrough')\n",
        "#connecting to matrix features X\n",
        "X=np.array(ct.fit_transform(X))\n",
        "#we need to have conversion in numpy for future training models"
      ],
      "metadata": {
        "id": "5EC1I_lToGbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5p4y48wqd50",
        "outputId": "1dd57282-6d4c-47b1-fde1-2c3841544f6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we will call  preprocessing module form sklearn again\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "Y=le.fit_transform(Y)  #This time we dont need to have numpy array as this is dependent variable vector for future models\n"
      ],
      "metadata": {
        "id": "HqPcifTnrXY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHqbku4zsUpd",
        "outputId": "72f73129-b991-44a8-c052-bce8baf9be78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using sklearn library for model selection which contains function train to split\n",
        "#So basicaly we are going to get four sets X_train(matrix of features of training set), X_test(matrix of features of test set),\n",
        "#Y_train(dependent variable of training set),Y_test(dependent variable of test set)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, Y, test_size=0.2,random_state=1) #We dont have to split dataset equally as we need to give a lot of data\n",
        "#to train for our model to understand correlations better in our dataset\n",
        "#Well to make sure we have same kind of random factors we just add random_state=1"
      ],
      "metadata": {
        "id": "YVsBvXNksv7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXFwwn9Uzicu",
        "outputId": "8be720f1-2d7b-46ba-852d-4d9af3fa4937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRbJR_j7zipj",
        "outputId": "f1e92705-f6ac-4e04-a2f5-245d7f0323d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpxRY_lHziym",
        "outputId": "41b71992-d3bf-4422-d18c-2425da095f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sItuA5xEzjBS",
        "outputId": "f99ec779-bb2d-4731-afa5-bb5189e353ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature scaling simply consists of scaling all the variables or your features actually to make sure they\n",
        "# all take values at same scale and we do this to prevent the dominance of one feature over the other which would be neglected\n",
        "#by machine learning model.\n",
        "#Feature scaling is simply a technique that will get the mean and the standard devaition of features in order to perform scaling.\n",
        "#So if we apply scaling before splitting then we actually get mean and standard deviation off all values including the values in\n",
        "#test set\n",
        "#And test set is supposed to have future data in production, you know applying future scaling on features in original data set \n",
        "# before the split would cost information leakage on the test set.\n",
        "\n",
        "#Feature scaling methods\n",
        "#1)Standardization 2)Normalization\n",
        "#1)Standardisation: xstand=(x-mean(x))/standard deviation(x)\n",
        "#2)Normalisation: xnorm=(x-min(x))/max(x)-min(x)\n",
        "\n",
        "#Normalisation is recommended when we have normal distribution in most of your features.\n",
        "#Standardisation works well all the time so this technique will work all the time because we will always do some relevant scaling of features and this will \n",
        "#improve training process\n",
        "#We will apply feature scaling not on X but on X_train and X_test separately and the scalar will fitted on X_train\n",
        "\n",
        "#StandardScalar class will be used for standardisation of matrix of features of X_train and X_test\n",
        "#Apply feature scalling to numerical values only not on dummy as we will loose information to which dummy variable corresspond to which country"
      ],
      "metadata": {
        "id": "I-kIvrd9tM7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc= StandardScaler()\n",
        "X_train[:,3:]=sc.fit_transform(X_train[:,3:])\n",
        "X_test[:,3:]=sc.transform(X_test[:,3:])"
      ],
      "metadata": {
        "id": "mzGRo6JkgNVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD89JvEOgujp",
        "outputId": "c9c33c67-fe35-48aa-b59a-f32dc5dee7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to apply same scalar on matrix features of X_test as we applied on X_train because X_test will be the input for predict function that will be used in machine learing model.\n",
        "So in order to make predictions that will be congruent with the way the model was trained so we need to apply the same scalar as of used in X_train."
      ],
      "metadata": {
        "id": "GjBVJqj6hVQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34Y97b9Og4vi",
        "outputId": "15ae24ff-6a21-4098-c6b3-6f10d56dcbe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lZav10ETjjTx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}